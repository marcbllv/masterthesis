\documentclass[a4paper,11pt]{kth-mag}

\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{nada-ex}

\graphicspath{{img/}}

\title{Inside the Black Box: How to Explain Individual Predictions of a Machine Learning Model}
\subtitle{How to automatically generate insights on predictive model outputs, and gain 
	a better understanding on how the model predicts each individual data point.}
\author{Marc Beillevaire}
\date{December 2016}
\blurb{
	Master's Thesis at CSC \\
	Supervisor: Hedvig Kjellstr√∂m \\
	Examiner: Danica Kragic
}
\trita{TRITA xxx yyyy-nn}
\begin{document}
\frontmatter
\maketitle


\begin{abstract}
Machine learning models are becoming more and more powerful and accurate, but their good predictions usually come with a high complexity. Depending on the situation, such a lack of interpretability can be an important and blocking issue. This is especially the case when trust is needed on the user side in order to take a decision based on the model prediction.

In this thesis, several explanation methods are described and compared on multiple datasets (text data, numerical), on classification and regression problems.
\end{abstract}

\clearpage
\selectlanguage{swedish}
\begin{abstract}
  SOME SWEDISH IN HERE....
\end{abstract}
\selectlanguage{english}
\clearpage
\tableofcontents
\mainmatter
\chapter{Introduction: what is an explanation ?}

Predictive models using machine learning are getting more and more efficient while also spreading in many area. In the industrial sector, many companies are starting to use their own data to optimize various processes. In online marketplaces customers' data are constantly processed by machine learning algorithms, whether it is for recommendation or to track potential fraudsters.

Yet, while most companies are eager to set up very efficient predictive models, a strong limitation usually still holds: when a human decision maker has to take action \textit{according to the model prediction}, trust in the model becomes at least as important as the accuracy of the prediction itself. People cannot willingly accept to take decision just because a computer said so, and they need to be provided with insights and explanations on why the model produced a given value.

This is the case in many areas: in fraud detection on an e-commerce website,  taking an action could mean adding another layer of security during a purchase like 3D security, which decreases the user experience and could lead to fewer purchases. The same apply to predictive maintenance models: maintenance operations on the equipment cost money, and such a predictive model would be put into production only if the company decision makers believe the algorithm can effectively predict the next failures in the equipment. Hence how efficient such models may be, they would only be applied if the business owners trusts them to effectively optimize the company's processes.

This need for insights on individual predictions is going to be more significant in the years to come in the European Union. Indeed in April 2016, the European Parliament adopted \href{http://eur-lex.europa.eu/eli/reg/2016/679/oj}{the General Data Protection Regulation (GDPR)}. Besides new rules regarding data prhttp://eur-lex.europa.eu/eli/reg/2016/679/ojotection, it provides every European citizen a \textit{right to explanation}: every decision having an impact on someone's life will have to be motivated by an explanation, even when the decision is taken by an algorithm. For instance, a bank agency won't be able to refuse a loan to an individual based only on an algorithmic decision.

This master thesis's goal will be, first, to define explanations more precisely. Then several solutions that proposes to explain predictions will be invastigated. Each of their benefits and limits will be compared and evaluated. Experiments will be made on various datasets, to assess the pertinence of each type of explanation.

\section{What are explanations ?}

Explaining a prediction in Machine Learning means give insights about the predictions of a model. Such insights should be informations about how the input values influenced the output produced by the model. 

Moreover, trust in a model comes with understandability. So, as these explanations should be read by humans, they must be easily understood by them. For instance a list of the most influencial inputs for a given prediction cannot contain 100 items, otherwise understandability and therefore trust are lost.

Thus an explanation can be defined as \textit{a set with a limited size of the features} that influenced the most a specific prediction. Along this set, \textit{an associated weight} to each variable can be added to assess quantitatively their relative influence on the prediction.

Yet, these influences can be defined by several ways.

\section{Contribution to the output value}

First, we can define the influence of the features as their contribution to the prediction. A good and understandable method would be to do so that summing all the influences gives the predictions. That would mean splitting the prediction into several part, and associating the influential features to the biggest parts and the less influential to the smallest.

For instance, let us consider a binary classification model for the Titanic dataset, that assess the probability between 0 and 1 that the person survives.
If for a given individual, the prediction is 80\% chances of survival, then the explanation could tell us that:

\begin{itemize}
\item being a woman, her survival probability increased by 32\%
\item being a first class passenger, it also increased by 15\%.
\item but being 47 years old, the probability of survival decreased by 3\%
\end{itemize}

This kind of explanation is described on figure 1.1. Here the model attempts to predict whether passengers from the Titanic have survived or not. It is based on a random forest with 250 trees, so examining all the trees to try to understand a single prediction is totally intractable by a human being.

In average, in the dataset, a passenger has an survival probability of $0.38$. But for this example the model predicts a survival probability of $0.93$ which is $0.55$ higher than the average value. This difference of $0.55$ is then only due to the feature values of the example. With this explanation, summing all the above contributions gives us the previous difference: $-0.03 - 0.02 + 0.01 + 0.02 + 0.11 + 0.15 + 0.32 = 0.55$.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{img/1-2-1.pdf_tex}
    \caption{Classification model on the Titanic dataset, explanation for a passenger, women, travelling in the first class, with a high ticket fare.}
\end{figure}

This way of building explanations produces very clear and understandable explanations. Indeed, it is very easy to understand that the \textit{age} feature has a negative contribution to the output probability of survival, while the \textit{Pclass} feature has a positive impact. Besides, it is easy to limit the set of important features by taking only the first $N$ relevant, and summing the rest of the influences into an additional \textit{"other features"} category.

Nevertheless, these types of explanations have a major drawback: even if for a single point it gives clear and understandable explanations, we have no idea of what happens when this point shifts a bit in the features space.

For instance we could be intereseted in knowing what happens when a given feature increases or deacreases. For a pricing prediction model, knowing which feature to change to increase the revenue is much more important than the actual output contribution.

\section{How the output changes}

\chapter{Practical methods and implementations}

Aliquam et ante.

\section{Model-dependent explanations}

\subsection{Linear models}

Vestibulum dolor dolor

\subsection{Tree based models}

Sed lobortis neque non mauris.

\section{Model-agnostic explanations}

\subsection{Lime}
Sed laoreet tellus in massa.

\subsection{Partial dependancies}
Sed laoreet tellus in massa.

\subsection{Other papers}
Sed laoreet tellus in massa.

\chapter{Experiments on datasets}
Aliquam quis nibh quis justo elementum viverra.

\section{Numerical datasets}
Suspendisse id mauris a justo venenatis feugiat.

\section{Categorical datasets}
Suspendisse id mauris a justo venenatis feugiat.

\section{Mixed types}
Suspendisse id mauris a justo venenatis feugiat.

\end{document}
\endinput
%%
%% End of file `kth-ex1.tex'.
